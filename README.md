
Deep Learning
=============

My musings related to deep learning.

Character-wise RNN for text generation
--------------------------------------

A sample from an RNN that trained on Anna Karenina:

> The steps, and the party
> started at the dining room, and the strange conversation was standing
> with a smile. The same words, the same though, and to bring a little
> shoulder, and the same smile, and the same strange face with which
> the priest and the past towards her, as though they were at the table,
> where a little girl would see how they had been the baby to them, and
> together at the table.
>
> "I'll tell you what I wanted to go to the country."
>
> "Well, then, I won't believe it," he said. "What am I to do? I want to
> do them, because I wanted to say..."
>
> "Yes," he said.
>
> "Well, then I'm natural and delicious, and I should not have thought of
> me, but that I could be so great the sake of me. If I do not ask you, to
> be a stranger and delightful tea on. The peasant was at a serious and
> the salary of him. It was an acquaintance."
>
> "Yes, yes," answered Levin, and stopped at the time with a
> smile, and the point was the only woman of the steps with his
> sharp arm and the carriage and the sharp health o

Embeddings - Skip-Gram
----------------------

Generate vector representations of words after infering meaning similarity from
the surrounding context.

![Embedding visualization](02-embeddings/assets/embedding.png)
